#############################################
# Spark SQL / Delta Lake
#############################################
# Delta integration
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Use external Hive Metastore
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=s3a://delta-lake/warehouse

#############################################
# Hive Metastore (Thrift HMS)
#############################################
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083

#############################################
# S3A (MinIO) â€” put LITERAL values here
# (Spark does NOT expand ${ENV} in this file)
#############################################
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=adminadmin
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

#############################################
# Hadoop Security Fix
#############################################
spark.hadoop.security.authentication=simple
spark.hadoop.ignore.secure.ports.for.testing=true

# Disable Hadoop security features that cause issues in containers
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.path.style.access=true

# Make sure JAAS/UGI has a non-empty username everywhere
spark.driver.extraJavaOptions=-Duser.name=root
spark.executor.extraJavaOptions=-Duser.name=root

spark.driver.host=airflow-scheduler
spark.driver.bindAddress=0.0.0.0
spark.driver.port=37113
spark.blockManager.port=47113

#############################################
# External packages (resolved by Ivy)
# Spark 3.5.1 (Scala 2.12) + Delta + Kafka + S3A
# Use Hadoop AWS 3.3.6 + AWS SDK v1 bundle 1.12.262
#############################################


#############################################
# IVY Configuration - Force local JARs only
#############################################
spark.jars=/opt/spark/jars/delta-spark_2.12-3.2.0.jar,/opt/spark/jars/delta-storage-3.2.0.jar
spark.jars.ivy=/tmp/ivy2
spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp/ivy2 -Divy.default.ivy.user.dir=/tmp/ivy2
spark.executor.extraJavaOptions=-Divy.cache.dir=/tmp/ivy2 -Divy.default.ivy.user.dir=/tmp/ivy2

# Disable remote repository resolution
spark.jars.repositories=
spark.jars.ivy.remote.repos=

# Disable package verification
spark.sql.packages.verification=false

#############################################
# Metrics / Prometheus
#############################################
spark.ui.prometheus.enabled=true
spark.metrics.conf=/opt/bitnami/spark/conf/metrics.properties

# (Optional) Event log / History Server
# spark.eventLog.enabled=true
# spark.eventLog.dir=s3a://delta-lake/spark-eventlog

# (Optional) Serializer + sizing defaults
# spark.serializer=org.apache.spark.serializer.KryoSerializer
# spark.executor.instances=2
# spark.executor.memory=2g
# spark.executor.cores=2
# spark.driver.memory=1g


