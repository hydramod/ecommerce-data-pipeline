#############################################
# Spark SQL / Delta Lake
#############################################
# Delta integration
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Use external Hive Metastore
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=s3a://delta-lake/warehouse

spark.sql.hive.metastore.version=3.1.3
spark.sql.hive.metastore.jars=/opt/spark/hive3/*

#############################################
# Hive Metastore (Thrift HMS)
#############################################
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083

#############################################
# Hive Security Configuration - CRITICAL FIX
#############################################
# Disable SASL authentication (causes delegation token errors)
spark.hadoop.hive.metastore.sasl.enabled=false
spark.hadoop.hive.metastore.token.signed.request.verification=false

# Disable delegation tokens entirely
spark.hadoop.hive.metastore.delegation.token.enabled=false
spark.hadoop.hive.metastore.token.signature=

# Disable Hive security authorization
spark.hadoop.hive.security.authorization.enabled=false
spark.hadoop.hive.metastore.execute.setugi=false

# Use simple authentication
spark.hadoop.hive.metastore.auth.type=none

#############################################
# S3A (MinIO) â€” put LITERAL values here
# (Spark does NOT expand ${ENV} in this file)
#############################################
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=adminadmin
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

#############################################
# Hadoop Security Fix
#############################################
spark.hadoop.security.authentication=simple
spark.hadoop.ignore.secure.ports.for.testing=true

spark.driver.host=airflow-scheduler
spark.driver.bindAddress=0.0.0.0
spark.driver.port=37113
spark.blockManager.port=47113

#############################################
# External packages (resolved by Ivy)
# Spark 3.5.1 (Scala 2.12) + Delta + Kafka + S3A
# Use Hadoop AWS 3.3.6 + AWS SDK v1 bundle 1.12.262
#############################################


#############################################
# IVY Configuration - Force local JARs only
#############################################
spark.jars=/opt/spark/jars/delta-spark_2.12-3.2.0.jar,/opt/spark/jars/delta-storage-3.2.0.jar
spark.jars.ivy=/tmp/ivy2
spark.driver.extraJavaOptions=-Duser.name=root -Divy.cache.dir=/tmp/ivy2 -Divy.default.ivy.user.dir=/tmp/ivy2
spark.executor.extraJavaOptions=-Duser.name=root -Divy.cache.dir=/tmp/ivy2 -Divy.default.ivy.user.dir=/tmp/ivy2

# Disable remote repository resolution
spark.jars.repositories=
spark.jars.ivy.remote.repos=

# Disable package verification
spark.sql.packages.verification=false

#############################################
# Metrics / Prometheus
#############################################
spark.ui.prometheus.enabled=true
spark.metrics.conf=/opt/bitnami/spark/conf/metrics.properties
