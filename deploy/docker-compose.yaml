services:
  gateway:
    image: traefik:v3.1
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
    ports: ["80:80","8080:8080"]
    depends_on: [auth, catalog, cart, order, payment, shipping, notifications]
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks: [mesh]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-appdb}
    ports: ["5432:5432"]
    volumes:
      - ./postgres-init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks: [mesh]
  
  adminer:
    image: adminer:4
    container_name: adminer
    restart: unless-stopped
    ports:
      - "8089:8080"          # http://localhost:8089
    depends_on:
      - postgres
    networks:
      - mesh

  redis:
    image: redis:7
    ports: ["6379:6379"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [mesh]

  zookeeper:
    image: bitnami/zookeeper:latest
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    networks: [mesh]

  kafka:
    image: bitnami/kafka:3.9
    environment:
      KAFKA_ENABLE_KRAFT: "no"    
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      ALLOW_PLAINTEXT_LISTENER: "yes"
    ports: ["9092:9092"]
    depends_on: [zookeeper]
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 15s
      timeout: 10s
      retries: 10
    networks: [mesh]

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY:-admin}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY:-adminadmin}
    ports: ["9000:9000","9001:9001"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks: [mesh]

  mailhog:
    image: mailhog/mailhog:v1.0.1
    ports: ["1025:1025","8025:8025"]
    networks: [mesh]

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    depends_on:
      - auth
      - catalog
      - order
      - cart
      - payment
      - shipping
      - notifications
      - postgres-exporter
      - redis-exporter
      - kafka-exporter
    networks: [mesh]
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks: [mesh]
    restart: unless-stopped

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_NAME: ${POSTGRES_EXPORTER_CONNSTRING:-postgresql://postgres:postgres@postgres:5432/appdb?sslmode=disable}
    ports: ["9187:9187"]
    depends_on: [postgres]
    networks: [mesh]

  redis-exporter:
    image: bitnami/redis-exporter:latest
    command: ["--redis.addr=redis://redis:6379"]
    ports: ["9121:9121"]
    depends_on: [redis]
    networks: [mesh]

  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    command:
      - "--kafka.server=kafka:9092"
    ports: ["9308:9308"]
    depends_on: [kafka]
    networks: [mesh]

  auth:
    build: ../services/auth
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      JWT_ALGORITHM: ${JWT_ALGORITHM}
      POSTGRES_DSN: ${POSTGRES_DSN}
    labels:
      - "traefik.http.routers.auth.rule=PathPrefix(`/auth`) || PathPrefix(`/users`)"
      - "traefik.http.routers.auth.entrypoints=web"
      - "traefik.http.services.auth.loadbalancer.server.port=8000"
    depends_on: [postgres]
    networks: [mesh]

  catalog:
    build: ../services/catalog
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      JWT_ALGORITHM: ${JWT_ALGORITHM}
      POSTGRES_DSN: ${POSTGRES_DSN}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET:-media}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
    labels:
      - "traefik.http.routers.catalog.rule=PathPrefix(`/catalog`)"
      - "traefik.http.routers.catalog.entrypoints=web"
      - "traefik.http.services.catalog.loadbalancer.server.port=8000"
    depends_on: 
      - postgres
      - minio
    networks: [mesh]

  cart:
    build: ../services/cart
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      REDIS_URL: ${REDIS_URL}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
      CATALOG_BASE: ${CATALOG_BASE:-http://catalog:8000}
    labels:
      - "traefik.http.routers.cart.rule=PathPrefix(`/cart`)"
      - "traefik.http.routers.cart.entrypoints=web"
      - "traefik.http.services.cart.loadbalancer.server.port=8000"
    depends_on: 
      - redis
      - catalog
    networks: [mesh]

  order:
    build: ../services/order
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      TOPIC_ORDER_EVENTS: ${TOPIC_ORDER_EVENTS:-orders.events}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payments.events}
      TOPIC_SHIPPING_EVENTS: ${TOPIC_SHIPPING_EVENTS:-shipping.events}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
      SHIPPING_BASE: http://shipping:8000
    labels:
      - "traefik.http.routers.order.rule=PathPrefix(`/order`)"
      - "traefik.http.routers.order.entrypoints=web"
      - "traefik.http.services.order.loadbalancer.server.port=8000"
    depends_on: 
      - postgres
      - kafka
    networks: [mesh]

  payment:
    build: ../services/payment
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payments.events}
      REDIS_URL: ${REDIS_URL}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
    labels:
      - "traefik.http.routers.payment.rule=PathPrefix(`/payment`)"
      - "traefik.http.routers.payment.entrypoints=web"
      - "traefik.http.services.payment.loadbalancer.server.port=8000"
    depends_on: 
      - postgres
      - kafka
      - redis
    networks: [mesh]

  shipping:
    build: ../services/shipping
    env_file: [.env]
    environment:
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      TOPIC_ORDER_EVENTS: ${TOPIC_ORDER_EVENTS:-orders.events}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payments.events}
      TOPIC_SHIPPING_EVENTS: ${TOPIC_SHIPPING_EVENTS:-shipping.events}
      JWT_SECRET: ${JWT_SECRET}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
    labels:
      - "traefik.http.routers.shipping.rule=PathPrefix(`/shipping`)"
      - "traefik.http.routers.shipping.entrypoints=web"
      - "traefik.http.services.shipping.loadbalancer.server.port=8000"
    depends_on: 
      - postgres
      - kafka
    networks: [mesh]

  notifications:
    build: ../services/notifications
    env_file: [.env]
    environment:
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      SMTP_HOST: ${SMTP_HOST:-mailhog}
      SMTP_PORT: ${SMTP_PORT:-1025}
      FROM_EMAIL: ${FROM_EMAIL:-no-reply@example.local}
      TOPIC_ORDER_EVENTS: ${TOPIC_ORDER_EVENTS:-orders.events}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payments.events}
      TOPIC_SHIPPING_EVENTS: ${TOPIC_SHIPPING_EVENTS:-shipping.events}
    labels:
      - "traefik.http.routers.notifications.rule=PathPrefix(`/notifications`)"
      - "traefik.http.routers.notifications.entrypoints=web"
      - "traefik.http.services.notifications.loadbalancer.server.port=8000"
    depends_on: 
      - kafka
      - mailhog
    networks: [mesh]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: ${KAFKA_BOOTSTRAP}
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181  # Optional, for better Zookeeper integration
    ports: ["8085:8080"]
    depends_on: [kafka, zookeeper]
    networks: [mesh]

  kafka-init:
    image: bitnami/kafka:latest
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - TOPIC_ORDER_EVENTS=orders.events
      - TOPIC_PAYMENT_EVENTS=payments.events
      - TOPIC_SHIPPING_EVENTS=shipping.events
    entrypoint: ["/bin/bash", "/deploy/kafka/init-topics.sh"]
    volumes:
      - ../deploy/kafka/init-topics.sh:/deploy/kafka/init-topics.sh
    depends_on:
      - kafka
    networks:
      - mesh

  hive-metastore:
    image: apache/hive:standalone-metastore-4.1.0
    user: "0:0"
    entrypoint: ["/bin/bash","-lc"]
    command: >
      "
      set -euo pipefail;
      curl -fSL -o /tmp/postgresql-42.7.7.jar https://jdbc.postgresql.org/download/postgresql-42.7.7.jar;
      export HADOOP_CLASSPATH=/tmp/postgresql-42.7.7.jar;
      /opt/hive/bin/schematool -dbType postgres -initOrUpgradeSchema -verbose || true;
      exec /opt/hive/bin/start-metastore
      "
    environment:
      HIVE_HOME: /opt/hive
      HIVE_CONF_DIR: /opt/hive/conf
    volumes:
      - ../analytics/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
    depends_on: [postgres, minio]
    ports: ["9083:9083"]
    networks: [mesh]

  spark-master:
    #build: ../analytics/spark
    image: ecom-spark
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_LOCAL_IP: spark-master 
      SPARK_NO_DAEMONIZE: true
      SPARK_SQL_EXTENSIONS: io.delta.sql.DeltaSparkSessionExtension
      SPARK_SQL_CATALOG: org.apache.spark.sql.delta.catalog.DeltaCatalog
      SPARK_EXTRA_CLASSPATH: /opt/bitnami/spark/jars/*
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: us-east-1
      S3_ENDPOINT: ${S3_ENDPOINT}
      FS_DEFAULTFS: ${LAKEHOUSE_URI:-s3a://delta-lake}
      FS_S3A_ENDPOINT: ${S3_ENDPOINT}
      FS_S3A_ACCESS_KEY: ${S3_ACCESS_KEY}
      FS_S3A_SECRET_KEY: ${S3_SECRET_KEY}
      FS_S3A_PATH_STYLE_ACCESS: ${S3A_PATH_STYLE_ACCESS:-true}
    volumes:
      - ../analytics/spark/conf:/opt/bitnami/spark/conf
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/spark/logs/master:/opt/bitnami/spark/logs
    ports: ["7077:7077","8081:8080"]
    depends_on: 
      - hive-metastore
      - kafka
      - kafka-init
    networks: [mesh]

  spark-worker:
    image: ecom-spark
    labels:
      - "traefik.enable=false"
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-2}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-2g}
      SPARK_NO_DAEMONIZE: "yes"
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: us-east-1
      S3_ENDPOINT: ${S3_ENDPOINT}
      FS_DEFAULTFS: ${LAKEHOUSE_URI:-s3a://delta-lake}
      FS_S3A_ENDPOINT: ${S3_ENDPOINT}
      FS_S3A_ACCESS_KEY: ${S3_ACCESS_KEY}
      FS_S3A_SECRET_KEY: ${S3_SECRET_KEY}
      FS_S3A_PATH_STYLE_ACCESS: ${S3A_PATH_STYLE_ACCESS:-true}
    volumes:
      - ../analytics/spark/conf:/opt/bitnami/spark/conf
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/spark/logs/worker:/opt/bitnami/spark/logs
      - ../analytics/spark/work/worker:/opt/bitnami/spark/work
    depends_on: 
      - spark-master
    networks: [mesh]

  airflow-init:
    #build: ../analytics/airflow
    image: ecom-airflow:latest
    user: "0:0"
    environment:
      USER: airflow
      HADOOP_USER_NAME: airflow
      SPARK_USER: airflow
      JAVA_TOOL_OPTIONS: "-Duser.name=airflow"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://postgres:postgres@postgres:5432/airflow}
      AIRFLOW_LOAD_EXAMPLES: "no"
      AIRFLOW_EXECUTOR: LocalExecutor
      # Keys (canonical + optional aliases)
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW_FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW_APISERVER_SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      # Execution API JWT secret
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      # Spark conn
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077?spark_binary=/opt/spark/bin/spark-submit&deploy-mode=client
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      # Misc
      AIRFLOW_WAIT_FOR_DATABASE: "yes"
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__LOGGING__MASK_SECRETS_IN_LOGS: "False"
      AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: "False"
    depends_on:
      - postgres
    networks: [mesh]
    volumes:
      - ../analytics/airflow/dags:/opt/bitnami/airflow/dags
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/airflow/logs:/opt/bitnami/airflow/logs
      - ./airflow-connections.yaml:/opt/bitnami/airflow/airflow-connections.yml:ro
      - ../analytics/spark/conf:/opt/bitnami/spark/conf
    command:
      - /bin/sh
      - -lc
      - >
        /opt/bitnami/airflow/venv/bin/airflow db migrate &&
        /opt/bitnami/airflow/venv/bin/airflow connections import /opt/bitnami/airflow/airflow-connections.yml


  airflow-webserver:
    image: ecom-airflow:latest
    user: "0:0"
    environment:
      USER: airflow
      HADOOP_USER_NAME: airflow
      SPARK_USER: airflow
      JAVA_TOOL_OPTIONS: "-Duser.name=airflow"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://postgres:postgres@postgres:5432/airflow}
      AIRFLOW_LOAD_EXAMPLES: "no"
      AIRFLOW_EXECUTOR: LocalExecutor
      # Keys (canonical + optional aliases)
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW_FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW_APISERVER_SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      # API config
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.jwt,airflow.api.auth.backend.session"
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      AIRFLOW__API__BASE_URL: http://airflow-webserver:8080
      AIRFLOW__API__HOST: 0.0.0.0
      AIRFLOW__API__PORT: "8080"
      # UI base URL
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8082
      # SimpleAuth for UI login
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: "admin:admin"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: "/opt/bitnami/airflow/logs/simple_auth_manager_passwords.json"
      # Spark conn
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077?spark_binary=/opt/spark/bin/spark-submit&deploy-mode=client
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      # Misc
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__LOGGING__MASK_SECRETS_IN_LOGS: "False"
      AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: "False"
    command: ["airflow", "api-server"]
    ports: ["8082:8080"]
    depends_on:
      - airflow-init
      - postgres
    networks: [mesh]
    volumes:
      - ../analytics/airflow/dags:/opt/bitnami/airflow/dags
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/airflow/logs:/opt/bitnami/airflow/logs
      - ../analytics/spark/conf:/opt/bitnami/spark/conf
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/health"]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 30s

  airflow-scheduler:
    image: ecom-airflow:latest
    user: "0:0"
    environment:
      USER: airflow
      HADOOP_USER_NAME: airflow
      SPARK_USER: airflow
      JAVA_TOOL_OPTIONS: "-Duser.name=airflow"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://postgres:postgres@postgres:5432/airflow}
      AIRFLOW_LOAD_EXAMPLES: "no"
      AIRFLOW_EXECUTOR: LocalExecutor
      # Keys (canonical + optional aliases)
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW_FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW_APISERVER_SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      # API client/server settings
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.jwt,airflow.api.auth.backend.session"
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      AIRFLOW__API__BASE_URL: http://airflow-webserver:8080
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      # SimpleAuth (not strictly needed here, harmless)
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: "admin:admin"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: "/opt/bitnami/airflow/logs/simple_auth_manager_passwords.json"
      # Spark conn (scheduler runs tasks under LocalExecutor)
      AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077?spark_binary=/opt/spark/bin/spark-submit&deploy-mode=client
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      AIRFLOW__LOGGING__MASK_SECRETS_IN_LOGS: "False"
      AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: "False"
    command: ["airflow", "scheduler"]
    depends_on:
      - airflow-init
      - postgres
      - airflow-webserver
    networks: [mesh]
    volumes:
      - ../analytics/airflow/dags:/opt/bitnami/airflow/dags
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/airflow/logs:/opt/bitnami/airflow/logs
      - ../analytics/spark/conf:/opt/bitnami/spark/conf

  airflow-triggerer:
    image: ecom-airflow:latest
    user: "0:0"
    environment:
      USER: airflow
      HADOOP_USER_NAME: airflow
      SPARK_USER: airflow
      JAVA_TOOL_OPTIONS: "-Duser.name=airflow"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://postgres:postgres@postgres:5432/airflow}
      AIRFLOW_LOAD_EXAMPLES: "no"
      AIRFLOW_EXECUTOR: LocalExecutor
      # Keys (canonical + optional aliases)
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW_FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW_APISERVER_SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      # API settings
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.jwt,airflow.api.auth.backend.session"
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      AIRFLOW__API__BASE_URL: http://airflow-webserver:8080
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      # (Spark connection not needed for triggerer, but harmless if left out)
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      AIRFLOW__LOGGING__MASK_SECRETS_IN_LOGS: "False"
      AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: "False"
    command: ["airflow", "triggerer"]
    depends_on:
      - airflow-init
      - postgres
      - airflow-webserver
    networks: [mesh]
    volumes:
      - ../analytics/airflow/dags:/opt/bitnami/airflow/dags
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/airflow/logs:/opt/bitnami/airflow/logs
      - ../analytics/spark/conf:/opt/bitnami/spark/conf

  airflow-dag-processor:
    image: ecom-airflow:latest
    user: "0:0"
    environment:
      USER: airflow
      HADOOP_USER_NAME: airflow
      SPARK_USER: airflow
      JAVA_TOOL_OPTIONS: "-Duser.name=airflow"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:-postgresql+psycopg2://postgres:postgres@postgres:5432/airflow}
      AIRFLOW_LOAD_EXAMPLES: "no"
      AIRFLOW_EXECUTOR: LocalExecutor
      # Keys (canonical + optional aliases)
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW_FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW_APISERVER_SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      # API settings
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.jwt,airflow.api.auth.backend.session"
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      AIRFLOW__API__BASE_URL: http://airflow-webserver:8080
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: "True"
      # (Spark connection not needed for dag-processor)
      SPARK_CONF_DIR: /opt/bitnami/spark/conf
      AIRFLOW__LOGGING__MASK_SECRETS_IN_LOGS: "False"
      AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: "False"
    command: ["airflow", "dag-processor"]
    depends_on:
      - airflow-init
      - postgres
      - airflow-webserver
    networks: [mesh]
    volumes:
      - ../analytics/airflow/dags:/opt/bitnami/airflow/dags
      - ../analytics/spark/jobs:/opt/jobs
      - ../analytics/airflow/logs:/opt/bitnami/airflow/logs
      - ../analytics/spark/conf:/opt/bitnami/spark/conf

  trino:
    image: trinodb/trino:443
    ports: ["${TRINO_HTTP_PORT:-8090}:8080"]
    environment:
      TRINO_CATALOG: ${TRINO_CATALOG:-delta}
      TRINO_SCHEMA: ${TRINO_SCHEMA:-default}
    volumes:
      - ../analytics/trino:/etc/trino
    depends_on: 
      - hive-metastore
    networks: [mesh]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/v1/info >/dev/null"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 15s

  metabase:
    image: metabase/metabase:latest
    ports: ["3001:3000"]
    environment:
      MB_DB_FILE: /metabase.db
    depends_on: 
      - trino
    networks: [mesh]

networks:
  mesh: {}

volumes:
  lake: {}