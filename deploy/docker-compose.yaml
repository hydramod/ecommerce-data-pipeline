services:
  gateway:
    image: traefik:v3.1
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
    ports: ["80:80","8080:8080"]
    depends_on: [auth, catalog, cart, order, payment, shipping, notifications]
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    networks: [mesh]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-appdb}
    ports: ["5432:5432"]
    networks: [mesh]

  redis:
    image: redis:7
    ports: ["6379:6379"]
    networks: [mesh]

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks: [mesh]

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports: ["9092:9092"]
    depends_on: [zookeeper]
    networks: [mesh]

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
    ports: ["9000:9000","9001:9001"]
    networks: [mesh]

  mailhog:
    image: mailhog/mailhog:v1.0.1
    ports: ["1025:1025","8025:8025"]
    networks: [mesh]

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      - auth
      - catalog
      - order
      - cart
      - payment
      - shipping
      - notifications
      - postgres-exporter
      - redis-exporter
      - kafka-exporter
    networks: [mesh]
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks: [mesh]
    restart: unless-stopped

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-appdb}?sslmode=disable"
    ports: ["9187:9187"]
    depends_on: [postgres]
    networks: [mesh]

  redis-exporter:
    image: bitnami/redis-exporter:latest
    command: ["--redis.addr=redis://redis:6379"]
    ports: ["9121:9121"]
    depends_on: [redis]
    networks: [mesh]

  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    command:
      - "--kafka.server=kafka:9092"
    ports: ["9308:9308"]
    depends_on: [kafka]
    networks: [mesh]

  auth:
    build: ../services/auth
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      JWT_ALGORITHM: ${JWT_ALGORITHM}
      POSTGRES_DSN: ${POSTGRES_DSN}
    labels:
      - "traefik.http.routers.auth.rule=PathPrefix(`/auth`) || PathPrefix(`/users`)"
      - "traefik.http.services.auth.loadbalancer.server.port=8000"
    depends_on: [postgres]
    networks: [mesh]

  catalog:
    build: ../services/catalog
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      JWT_ALGORITHM: ${JWT_ALGORITHM}
      POSTGRES_DSN: ${POSTGRES_DSN}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
    labels:
      - "traefik.http.routers.catalog.rule=PathPrefix(`/catalog`)"
      - "traefik.http.services.catalog.loadbalancer.server.port=8000"
    depends_on: [postgres, minio]
    networks: [mesh]

  cart:
    build: ../services/cart
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      REDIS_URL: ${REDIS_URL}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
      CATALOG_BASE: ${CATALOG_BASE:-http://gateway/catalog}
    labels:
      - "traefik.http.routers.cart.rule=PathPrefix(`/cart`)"
      - "traefik.http.services.cart.loadbalancer.server.port=8000"
    depends_on: [redis, catalog]
    networks: [mesh]

  order:
    build: ../services/order
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_ANALYTICS_TOPIC_ORDERS: ${KAFKA_ANALYTICS_TOPIC_ORDERS:-orders.v1}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
      SHIPPING_BASE: http://shipping:8000
    labels:
      - "traefik.http.routers.order.rule=PathPrefix(`/order`)"
      - "traefik.http.services.order.loadbalancer.server.port=8000"
    depends_on: [postgres, kafka]
    networks: [mesh]

  payment:
    build: ../services/payment
    env_file: [.env]
    environment:
      JWT_SECRET: ${JWT_SECRET}
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_ANALYTICS_TOPIC_PAYMENTS: ${KAFKA_ANALYTICS_TOPIC_PAYMENTS:-payments.v1}
      REDIS_URL: ${REDIS_URL}
      SVC_INTERNAL_KEY: ${SVC_INTERNAL_KEY}
    labels:
      - "traefik.http.routers.payment.rule=PathPrefix(`/payment`)"
      - "traefik.http.services.payment.loadbalancer.server.port=8000"
    depends_on: [postgres, kafka, redis]
    networks: [mesh]

  shipping:
    build: ../services/shipping
    env_file: [.env]
    environment:
      POSTGRES_DSN: ${POSTGRES_DSN}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      TOPIC_ORDER_EVENTS: ${TOPIC_ORDER_EVENTS:-order.events}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payment.events}
      TOPIC_SHIPPING_EVENTS: ${TOPIC_SHIPPING_EVENTS:-shipping.events}
    labels:
      - "traefik.http.routers.shipping.rule=PathPrefix(`/shipping`)"
      - "traefik.http.services.shipping.loadbalancer.server.port=8000"
    depends_on: [postgres, kafka]
    networks: [mesh]

  notifications:
    build: ../services/notifications
    env_file: [.env]
    environment:
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      SMTP_HOST: ${SMTP_HOST:-mailhog}
      SMTP_PORT: ${SMTP_PORT:-1025}
      FROM_EMAIL: ${FROM_EMAIL:-no-reply@example.local}
      TOPIC_ORDER_EVENTS: ${TOPIC_ORDER_EVENTS:-order.events}
      TOPIC_PAYMENT_EVENTS: ${TOPIC_PAYMENT_EVENTS:-payment.events}
      TOPIC_SHIPPING_EVENTS: ${TOPIC_SHIPPING_EVENTS:-shipping.events}
    labels:
      - "traefik.http.routers.notifications.rule=PathPrefix(`/notifications`)"
      - "traefik.http.services.notifications.loadbalancer.server.port=8000"
    depends_on: [kafka, mailhog]
    networks: [mesh]

    kafka-ui:
      image: provectuslabs/kafka-ui:latest
      environment:
        KAFKA_CLUSTERS_0_NAME: local
        KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: ${KAFKA_BOOTSTRAP}
      ports: ["8085:8080"]
      depends_on: [kafka]
      networks: [mesh]

  # Create analytics topics if they don't exist
  kafka-init:
    image: confluentinc/cp-kafka:7.6.0
    depends_on: [kafka]
    entrypoint: ["/bin/bash","-lc"]
    command: >
      "
      cub kafka-ready -b ${KAFKA_BOOTSTRAP} 1 60 &&
      kafka-topics --bootstrap-server ${KAFKA_BOOTSTRAP} --create --if-not-exists
        --topic ${KAFKA_ANALYTICS_TOPIC_ORDERS:-orders.v1}   --partitions 6 --replication-factor 1 &&
      kafka-topics --bootstrap-server ${KAFKA_BOOTSTRAP} --create --if-not-exists
        --topic ${KAFKA_ANALYTICS_TOPIC_PAYMENTS:-payments.v1} --partitions 6 --replication-factor 1
      "
    networks: [mesh]

  hive-metastore:
    image: bitsondatadev/hive-metastore:3.1.3
    environment:
      METASTORE_DB_TYPE: postgres
      METASTORE_DB_HOST: postgres
      METASTORE_DB_NAME: ${POSTGRES_DB}
      METASTORE_DB_USER: ${POSTGRES_USER}
      METASTORE_DB_PASS: ${POSTGRES_PASSWORD}
    ports: ["9083:9083"]
    depends_on: [postgres]
    networks: [mesh]
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9083 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20

    spark-master:
      image: bitnami/spark:3.5
      environment:
        SPARK_MODE: master
        SPARK_NO_DAEMONIZE: "yes"
        # Delta + Hive + S3A
        SPARK_SQL_EXTENSIONS: io.delta.sql.DeltaSparkSessionExtension
        SPARK_SQL_CATALOG: org.apache.spark.sql.delta.catalog.DeltaCatalog
        SPARK_PUBLIC_DNS: spark-master
        # S3A (MinIO)
        SPARK_EXTRA_CLASSPATH: /opt/bitnami/spark/jars/*
        SPARK_CONF_DIR: /opt/bitnami/spark/conf
        AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
        AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
        AWS_REGION: us-east-1
        S3_ENDPOINT: ${S3_ENDPOINT}
      volumes:
        - ./analytics/spark/conf:/opt/bitnami/spark/conf:ro
        - ./analytics/jobs:/opt/jobs:ro
      ports: ["7077:7077","8081:8080"]
      depends_on: [hive-metastore, kafka, kafka-init]
      networks: [mesh]

    spark-worker:
      image: bitnami/spark:3.5
      environment:
        SPARK_MODE: worker
        SPARK_MASTER_URL: spark://spark-master:7077
        SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-2}
        SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-2g}
        SPARK_NO_DAEMONIZE: "yes"
        SPARK_CONF_DIR: /opt/bitnami/spark/conf
        AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
        AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
        AWS_REGION: us-east-1
        S3_ENDPOINT: ${S3_ENDPOINT}
      volumes:
        - ./analytics/spark/conf:/opt/bitnami/spark/conf:ro
        - ./analytics/jobs:/opt/jobs:ro
      depends_on: [spark-master]
      networks: [mesh]

    airflow:
      image: apache/airflow:2.9.3
      environment:
        AIRFLOW__CORE__LOAD_EXAMPLES: "false"
        AIRFLOW__CORE__EXECUTOR: LocalExecutor
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
        _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-admin}
        _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-admin}
      user: "0:0"
      command: >
        bash -lc '
        airflow db init &&
        airflow users create --role Admin --username "${_AIRFLOW_WWW_USER_USERNAME:-admin}" --password "${_AIRFLOW_WWW_USER_PASSWORD:-admin}" --firstname a --lastname b --email a@b.c || true &&
        airflow webserver -p 8080 & airflow scheduler
        '
      volumes:
        - ./analytics/dags:/opt/airflow/dags:ro
        - ./analytics/jobs:/opt/jobs:ro
      ports: ["8082:8080"]
      depends_on: [spark-master]
      networks: [mesh]

    trino:
      image: trinodb/trino:443
      ports: ["8090:8080"]
      volumes:
        - ./analytics/trino/etc:/etc/trino:ro
      depends_on: [hive-metastore]
      networks: [mesh]

    metabase:
      image: metabase/metabase:latest
      ports: ["3000:3000"]
      environment:
        MB_DB_FILE: /metabase.db
      depends_on: [trino]
      networks: [mesh]

networks:
  mesh: {}
volumes:
  lake: {}
